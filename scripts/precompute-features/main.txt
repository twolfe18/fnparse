
# How to run all the scripts in this directory by hand.
# By hand because it needs to be asyncronous (qsub takes a while and doesn't block)



DATASET=propbank
NUM_ROLES=30
#WORKING_DIR=experiments/precompute-features/propbank/sep14a/raw-shards
#WORKING_DIR=/export/projects/twolfe/fnparse-output/experiments/precompute-features/propbank/sep14b/raw-shards
#WORKING_DIR=/export/projects/twolfe/fnparse-output/experiments/precompute-features/framenet/sep29a/raw-shards
#WORKING_DIR=/export/projects/twolfe/fnparse-output/experiments/for-oct-tacl/framenet/oct21a/
WORKING_DIR=/export/projects/twolfe/fnparse-output/experiments/for-oct-tacl/$DATASET/oct21a/
JAR=target/fnparse-1.0.6-SNAPSHOT-jar-with-dependencies.jar

############################################################################################
### CREATE MANY SHARDS OF FEATURES

# Start redis parse servers.
# These slightly an accident of history with some meh reasons:
# 1) I didn't originally serialize the parses with the Propbank data
# 2) I wanted a way to cache parses across machines
# 3) I can fit all of the FNParses in memory, but not with the parses as well
qsub -N parse-fPreComp ./scripts/propbank-train-redis-parse-server.sh
qsub -N parse-fPreComp ./scripts/propbank-train-redis-parse-server.sh
qsub -N parse-fPreComp ./scripts/propbank-train-redis-parse-server.sh
qsub -N parse-fPreComp ./scripts/propbank-train-redis-parse-server.sh
# Now look up the machines these were dispatched to and copy those namaes into the array below.

NUM_SHARDS=500
PARSE_REDIS_SERVERS=(r5n28 r6n23 r5n09 r6n31)

mkdir -p $WORKING_DIR/raw-shards/sge-logs

echo "copyinig jar to a safe place..."
JAR_STABLE=$WORKING_DIR/raw-shards/fnparse.jar
echo "    $JAR"
echo "==> $JAR_STABLE"
cp $JAR $JAR_STABLE

echo "starting at `date`" >>$WORKING_DIR/raw-shards/commands.txt
for i in `seq $NUM_SHARDS | awk '{print $1 - 1}'`; do
  WD=$WORKING_DIR/raw-shards/job-$i-of-$NUM_SHARDS
  J=`echo $i | awk '{print $1 % 4}'`
  PS=${PARSE_REDIS_SERVERS[$J]}
  echo "dispatching to $PS"
  mkdir -p $WD
  COMMAND="qsub -N fPreComp-$i-$NUM_SHARDS \
    -o $WORKING_DIR/raw-shards/sge-logs \
    scripts/precompute-features/extract-one-shard.sh \
      $WD \
      $JAR_STABLE \
      $i \
      $NUM_SHARDS \
      $PS \
      $DATASET"
   echo $COMMAND >>$WORKING_DIR/raw-shards/commands.txt
   eval $COMMAND
done


############################################################################################
### MERGE MANY SHARDS INTO A COHERENT ALPHABET
#WORKING_DIR=/export/projects/twolfe/fnparse-output/experiments/precompute-features/framenet/sep29a
python -u scripts/precompute-features/bialph-merge-pipeline.py \
  $WORKING_DIR \
  $NUM_SHARDS \
  | tee /tmp/merge-job-launch-log.txt


############################################################################################
### CREATE TEMPLATE FILTERS
# e.g. "only have a feature fire if it is in the top K most frequent by this template"
# or "only have this feature fire if is has fired at least K other times in the corpus"
# These features will be put at the end of every line in the feature files and a new
# alphabet will be writte out including them.

# A) Compute the feature frequencies
BIALPH_BEFORE_FILTERS=$WORKING_DIR/coherent-shards/alphabet.txt.gz
FEATS_BEFORE_FILTERS=$WORKING_DIR/coherent-shards/features
mkdir -p $WORKING_DIR/feature-counts/sge-logs

echo "copyinig jar to a safe place..."
JAR_STABLE=$WORKING_DIR/feature-counts/fnparse.jar
echo "    $JAR"
echo "==> $JAR_STABLE"
cp $JAR $JAR_STABLE

qsub -N featCounts-full \
  -b y -j y -cwd -V -o $WORKING_DIR/feature-counts/sge-logs \
  -l 'mem_free=25G,num_proc=1,h_rt=12:00:00' \
  java -cp $JAR_STABLE -Xmx24G -ea -server \
    -Dbialph=$BIALPH_BEFORE_FILTERS \
    -Doutput=$WORKING_DIR/feature-counts/all.txt.gz \
    -DfeaturesParent=$FEATS_BEFORE_FILTERS \
    edu.jhu.hlt.fnparse.features.precompute.FeatureCounts
qsub -N featCounts-tenth \
  -b y -j y -cwd -V -o $WORKING_DIR/feature-counts/sge-logs \
  -l 'mem_free=25G,num_proc=1,h_rt=12:00:00' \
  java -cp $JAR_STABLE -Xmx15G -ea -server \
    -Dbialph=$BIALPH_BEFORE_FILTERS \
    -Doutput=$WORKING_DIR/feature-counts/one-tenth.txt.gz \
    -DfeaturesGlob="glob:**/shard*9.txt.gz" \
    -DfeaturesParent=$FEATS_BEFORE_FILTERS \
    edu.jhu.hlt.fnparse.features.precompute.FeatureCounts
qsub -N featCounts-hundreth \
  -b y -j y -cwd -V -o $WORKING_DIR/feature-counts/sge-logs \
  -l 'mem_free=25G,num_proc=1,h_rt=12:00:00' \
  java -cp $JAR_STABLE -Xmx15G -ea -server \
    -Dbialph=$BIALPH_BEFORE_FILTERS \
    -Doutput=$WORKING_DIR/feature-counts/one-hundredth.txt.gz \
    -DfeaturesGlob="glob:**/shard*99.txt.gz" \
    -DfeaturesParent=$FEATS_BEFORE_FILTERS \
    edu.jhu.hlt.fnparse.features.precompute.FeatureCounts


# B) Generate the filtered features based on the frequencies
#   (if i try to do ~60G => +filters:~8x => 250G with one process... it is going to take a while...)
mkdir -p $WORKING_DIR/coherent-shards-filtered/sge-logs
mkdir -p $WORKING_DIR/coherent-shards-filtered/features
BIALPH_AFTER_FILTERS=$WORKING_DIR/coherent-shards-filtered/alphabet.txt.gz

# Make sure this is done!
#COUNT_FILE=$WORKING_DIR/feature-counts/one-tenth.txt.gz
COUNT_FILE=$WORKING_DIR/feature-counts/all.txt.gz

echo "copyinig jar to a safe place..."
JAR_STABLE=$WORKING_DIR/coherent-shards-filtered/fnparse.jar
echo "    $JAR"
echo "==> $JAR_STABLE"
cp $JAR $JAR_STABLE

# Takes shard 0 and builds output bialph
qsub -N genFiltered-alph \
  -b y -j y -cwd -V -o $WORKING_DIR/coherent-shards-filtered/sge-logs \
  -l 'mem_free=20G,num_proc=1,h_rt=12:00:00' \
  java -cp $JAR_STABLE -Xmx19G -ea -server \
    -Dshard=0 \
    -DnumShards=$NUM_SHARDS \
    -DcountFile=$COUNT_FILE \
    -DoutputBialph=$BIALPH_AFTER_FILTERS \
    -Dbialph=$BIALPH_BEFORE_FILTERS \
    -DoutputFeatureFileDir=$WORKING_DIR/coherent-shards-filtered/features \
    -DfeaturesGlob="glob:**/*" \
    -DfeaturesParent=$WORKING_DIR/coherent-shards/features \
    edu.jhu.hlt.fnparse.features.precompute.TemplateTransformer
for SHARD in `echo "$NUM_SHARDS-1" | bc | xargs seq`; do    # takes shards 1..
  qsub -N genFiltered \
    -b y -j y -cwd -V -o $WORKING_DIR/coherent-shards-filtered/sge-logs \
    -l 'mem_free=20G,num_proc=1,h_rt=12:00:00' \
    java -cp $JAR_STABLE -Xmx19G -ea -server \
      -Dshard=$SHARD \
      -DnumShards=$NUM_SHARDS \
      -DcountFile=$COUNT_FILE \
      -Dbialph=$BIALPH_BEFORE_FILTERS \
      -DoutputFeatureFileDir=$WORKING_DIR/coherent-shards-filtered/features \
      -DfeaturesGlob="glob:**/*" \
      -DfeaturesParent=$WORKING_DIR/coherent-shards/features \
      edu.jhu.hlt.fnparse.features.precompute.TemplateTransformer
done


############################################################################################
### COMPUTE INFORMATION GAIN
#WD=/export/projects/twolfe/fnparse-output/experiments/precompute-features/propbank/sep14b
#WD=/export/projects/twolfe/fnparse-output/experiments/precompute-features/framenet/sep29a

# 0) Find the sentence ids of the test set and don't use these for computing information gain
TEST_SET_SENT_IDS="$WORKING_DIR/test-set-sentence-ids.txt"
mvn exec:java \
  -Dexec.mainClass=edu.jhu.hlt.fnparse.features.precompute.DumpSentenceIds \
  -Ddata.ontonotes5=data/ontonotes-release-5.0/LDC2013T19/data/files/data/english/annotations \
  -Ddata.propbank.conll=../conll-formatted-ontonotes-5.0/conll-formatted-ontonotes-5.0/data \
  -Ddata.propbank.frames=data/ontonotes-release-5.0-fixed-frames/frames \
  -Ddata=$DATASET \
  -Dpart=test \
  -Doutput=$TEST_SET_SENT_IDS

# 1) For each template
mkdir -p $WORKING_DIR/ig/templates/sge-logs

JAR=target/fnparse-1.0.6-SNAPSHOT-jar-with-dependencies.jar
JAR_STABLE=$WORKING_DIR/ig/templates/fnparse.jar
echo "copying the jar to a safe place..."
echo "    $JAR"
echo "==> $JAR_STABLE"
cp $JAR $JAR_STABLE

# Do a 1/10 estimate of 1/10 of the data and average the answers (should take 1/10th the time but be approximate)
N=10
XMX=16
XMX_SGE=18
mkdir -p $WORKING_DIR/ig/templates/split-$N-filter10
for i in `seq $N | awk '{print $1-1}'`; do
  qsub -N split$N-filter10-$i-ig-template \
    -l "mem_free=${XMX_SGE}G" \
    -o $WORKING_DIR/ig/templates/sge-logs \
    scripts/precompute-features/compute-ig.sh \
      $WORKING_DIR/coherent-shards-filtered/features \
      "glob:**/*1${i}.txt.gz" \
      $WORKING_DIR/ig/templates/split-$N-filter10/shard-${i}.txt \
      $JAR_STABLE \
      $TEST_SET_SENT_IDS \
      $NUM_ROLES \
      $XMX
done

# Do a 1/10 estimate and average the answers (should take 1/10th the time but be approximate)
N=10
XMX=20
XMX_SGE=22
mkdir -p $WORKING_DIR/ig/templates/split-$N
for i in `seq $N | awk '{print $1-1}'`; do
  qsub -N split$N-$i-ig-template \
    -l "mem_free=${XMX_SGE}G" \
    -o $WORKING_DIR/ig/templates/sge-logs \
    scripts/precompute-features/compute-ig.sh \
      $WORKING_DIR/coherent-shards-filtered/features \
      "glob:**/*${i}.txt.gz" \
      $WORKING_DIR/ig/templates/split-$N/shard-${i}.txt \
      $JAR_STABLE \
      $TEST_SET_SENT_IDS \
      $NUM_ROLES \
      $XMX
done

# Do a full/proper estimate
XMX=30
XMX_SGE=32
qsub -N full-ig-template \
  -l "mem_free=${XMX_SGE}G" \
  -o $WORKING_DIR/ig/templates/sge-logs \
  scripts/precompute-features/compute-ig.sh \
    $WORKING_DIR/coherent-shards-filtered/features \
    "glob:**/*" \
    $WORKING_DIR/ig/templates/full-ig.txt \
    $JAR_STABLE \
    $TEST_SET_SENT_IDS \
    $NUM_ROLES \
    $XMX


### After those jobs finish, average things
python scripts/precompute-features/average-ig.py \
  $WORKING_DIR/ig/templates/split-$N/shard-* \
  >$WORKING_DIR/ig/templates/split-$N/average.txt


# 2) For prodcuts of templates, filtered by top K products ranked by product of template IG
# NOTE: The more shards the more products we compute on
TEMPLATE_IG_FILE=$WORKING_DIR/ig/templates/split-$N/average.txt
#TEMPLATE_IG_FILE=$WORKING_DIR/ig/templates/full-ig.txt
PROD_IG_WD=$WORKING_DIR/ig/products
mkdir -p $PROD_IG_WD/ig-files
mkdir -p $PROD_IG_WD/sge-logs
cp target/fnparse-1.0.6-SNAPSHOT-jar-with-dependencies.jar $PROD_IG_WD/fnparse.jar
NUM_SHARDS=500
FEATS_PER_SHARD=500
for i in `seq 221 $NUM_SHARDS | awk '{print $1 - 1}'`; do
  qsub -N prod-ig-$i -o $PROD_IG_WD/sge-logs \
    ./scripts/precompute-features/compute-ig-products.sh \
      $i \
      $NUM_SHARDS \
      $FEATS_PER_SHARD \
      $TEMPLATE_IG_FILE \
      $WORKING_DIR/coherent-shards-filtered/features \
      "glob:**/*" \
      $WORKING_DIR/coherent-shards-filtered/alphabet.txt.gz \
      $TEST_SET_SENT_IDS \
      $PROD_IG_WD/ig-files/shard-${i}-of-${NUM_SHARDS}.txt \
      $PROD_IG_WD/fnparse.jar \
      $NUM_ROLES
done





############################################################################################
### Build feature sets
# see Makefile in scripts/having-a-laugh

DATASET=framenet
WORKING_DIR=/export/projects/twolfe/fnparse-output/experiments/for-oct-tacl/$DATASET/oct21a/

## Build a filtered bilaph
# After all the filters, these alphabets will have hundreds of millions of
# features and will be hundreds of MB after gzipping. The bialphs which contain
# all the templates selected by feature selection can be less than a million
# feature (order of hundreds of templates).
# This step just removes unused templates from the full bialph.
BIALPH_FULL=$WORKING_DIR/coherent-shards-filtered/alphabet.txt.gz
BIALPH_SMALL=$WORKING_DIR/coherent-shards-filtered/alphabet.onlyTemplatesInFs.txt
#zcat $BIALPH_FULL \
#  | python scripts/having-a-laugh/filter_bialph.py \
#    /dev/stdin \
#    $BIALPH_SMALL \
#    scripts/having-a-laugh/${DATASET}-[1-9]*.fs
java -cp target/fnparse-1.0.6-SNAPSHOT-jar-with-dependencies.jar -ea -server edu.jhu.hlt.fnparse.features.precompute.BiAlphFilter $BIALPH_FULL $BIALPH_SMALL scripts/having-a-laugh/${DATASET}-8-640.fs scripts/having-a-laugh/${DATASET}-16-640.fs scripts/having-a-laugh/${DATASET}-32-640.fs


############################################################################################
# TODO Embed features?
# Just run word2vec on my feature files (after filtering by top K templates by MI maybe)?
# Pretty sure the embeddings will not be linear with the weights,
#   will want some type of RBF kernel perceptron (average alpha*K(x,x') over all features for a given instance as decision rule).




############################################################################################
### RUN THE ACTUAL EXPERIMENTS

# 1) see Makefile in scripts/having-a-laugh (build feature sets based on product IG + dedup)
# 2) launch experiments with scripts/precompute-features/train-all.sh

#WD=/export/projects/twolfe/fnparse-output/experiments/for-sept-tacl/propbank
#WD=/export/projects/twolfe/fnparse-output/experiments/precompute-features/framenet/sep29a/experiments/dropoutY-noreg
#DD=/export/projects/twolfe/fnparse-output/experiments/precompute-features/framenet/sep29a

#WD=/export/projects/twolfe/fnparse-output/experiments/for-oct-tacl/framenet/oct21a/experiments-nov7
#DD=/export/projects/twolfe/fnparse-output/experiments/for-oct-tacl/framenet/oct21a
#IS_PROPBANK="false"
#DEFAULT_DIM=32-160
WD=/export/projects/twolfe/fnparse-output/experiments/for-oct-tacl/propbank/oct21a/experiments-nov7
DD=/export/projects/twolfe/fnparse-output/experiments/for-oct-tacl/propbank/oct21a
IS_PROPBANK="true"
DEFAULT_DIM=32-640

DEFAULT_ORACLE="RAND_MIN"
DEFAULT_BEAM=1
DEFAULT_NTRAIN=0
DEFAULT_REG=8

#TAG="k-"
TAG=""

# Regularizer strength
BEAM_SIZE=$DEFAULT_BEAM
ORACLE_MODE=$DEFAULT_ORACLE
DIM=$DEFAULT_DIM
FORCE_LEFT_RIGHT="false"
PERCEPTRON="false"
NTRAIN=$DEFAULT_NTRAIN
REG="#"
./scripts/precompute-features/train-all.sh \
  $WD/sweep-reg \
  $DD \
  $IS_PROPBANK \
  $DIM \
  $ORACLE_MODE \
  $BEAM_SIZE \
  $FORCE_LEFT_RIGHT \
  $PERCEPTRON \
  $NTRAIN \
  $REG \
  "${TAG}R"

# Oracle mode
BEAM_SIZE=$DEFAULT_BEAM
ORACLE_MODE="#"
DIM=$DEFAULT_DIM
FORCE_LEFT_RIGHT="false"
PERCEPTRON="false"
NTRAIN=$DEFAULT_NTRAIN
REG=$DEFAULT_REG
./scripts/precompute-features/train-all.sh \
  $WD/sweep-oracle \
  $DD \
  $IS_PROPBANK \
  $DIM \
  $ORACLE_MODE \
  $BEAM_SIZE \
  $FORCE_LEFT_RIGHT \
  $PERCEPTRON \
  $NTRAIN \
  $REG \
  "${TAG}N"

# Dimension / feature set
BEAM_SIZE=$DEFAULT_BEAM
ORACLE_MODE=$DEFAULT_ORACLE
DIM="#"
FORCE_LEFT_RIGHT="false"
PERCEPTRON="false"
NTRAIN=$DEFAULT_NTRAIN
REG=$DEFAULT_REG
./scripts/precompute-features/train-all.sh \
  $WD/sweep-dim \
  $DD \
  $IS_PROPBANK \
  $DIM \
  $ORACLE_MODE \
  $BEAM_SIZE \
  $FORCE_LEFT_RIGHT \
  $PERCEPTRON \
  $NTRAIN \
  $REG \
  "${TAG}D"

# Force left right
# and perceptron
BEAM_SIZE=$DEFAULT_BEAM
ORACLE_MODE=$DEFAULT_ORACLE
DIM=$DEFAULT_DIM
FORCE_LEFT_RIGHT="#"
PERCEPTRON="#"
NTRAIN=$DEFAULT_NTRAIN
REG=$DEFAULT_REG
./scripts/precompute-features/train-all.sh \
  $WD/sweep-perceptron \
  $DD \
  $IS_PROPBANK \
  $DIM \
  $ORACLE_MODE \
  $BEAM_SIZE \
  $FORCE_LEFT_RIGHT \
  $PERCEPTRON \
  $NTRAIN \
  $REG \
  "${TAG}L"

# Beam size
BEAM_SIZE="#"
ORACLE_MODE=$DEFAULT_ORACLE
DIM=$DEFAULT_DIM
FORCE_LEFT_RIGHT="false"
PERCEPTRON="false"
NTRAIN=$DEFAULT_NTRAIN
REG=$DEFAULT_REG
./scripts/precompute-features/train-all.sh \
  $WD/sweep-beam \
  $DD \
  $IS_PROPBANK \
  $DIM \
  $ORACLE_MODE \
  $BEAM_SIZE \
  $FORCE_LEFT_RIGHT \
  $PERCEPTRON \
  $NTRAIN \
  $REG \
  "${TAG}B"

# Train size
BEAM_SIZE=$DEFAULT_BEAM
ORACLE_MODE=$DEFAULT_ORACLE
DIM=$DEFAULT_DIM
FORCE_LEFT_RIGHT="false"
PERCEPTRON="false"
NTRAIN="#"
REG=$DEFAULT_REG
./scripts/precompute-features/train-all.sh \
  $WD/sweep-size \
  $DD \
  $IS_PROPBANK \
  $DIM \
  $ORACLE_MODE \
  $BEAM_SIZE \
  $FORCE_LEFT_RIGHT \
  $PERCEPTRON \
  $NTRAIN \
  $REG \
  "${TAG}S"

qinfo

# vim: set syntax=sh
