
# How to run all the scripts in this directory by hand.
# By hand because it needs to be asyncronous (qsub takes a while and doesn't block)


############################################################################################
### CREATE MANY SHARDS OF FEATURES

# Start redis parse servers.
# These slightly an accident of history with some meh reasons:
# 1) I didn't originally serialize the parses with the Propbank data
# 2) I wanted a way to cache parses across machines
# 3) I can fit all of the FNParses in memory, but not with the parses as well
#qsub -N parse-fPreComp ./scripts/propbank-train-redis-parse-server.sh
#qsub -N parse-fPreComp ./scripts/propbank-train-redis-parse-server.sh
#qsub -N parse-fPreComp ./scripts/propbank-train-redis-parse-server.sh
#qsub -N parse-fPreComp ./scripts/propbank-train-redis-parse-server.sh
# Now look up the machines these were dispatched to and copy those namaes into the array below.


#WORKING_DIR=experiments/precompute-features/propbank/sep14a/raw-shards
WORKING_DIR=/export/projects/twolfe/fnparse-output/experiments/precompute-features/propbank/sep14b/raw-shards
JAR=target/fnparse-1.0.6-SNAPSHOT-jar-with-dependencies.jar
NUM_SHARDS=400
PARSE_REDIS_SERVERS=(r8n07 r8n08 r8n11 r8n12)

mkdir -p $WORKING_DIR/sge-logs

echo "copyinig jar to a safe place..."
JAR_STABLE=$WORKING_DIR/fnparse.jar
echo "    $JAR"
echo "==> $JAR_STABLE"
cp $JAR $JAR_STABLE

# TODO move `cp $JAR $JAR_STABLE` up to this script
echo "starting at `date`" >>$WORKING_DIR/commands.txt
for i in `seq $NUM_SHARDS | awk '{print $1 - 1}'`; do
  WD=$WORKING_DIR/job-$i-of-$NUM_SHARDS
  J=`echo $i | awk '{print $1 % 4}'`
  PS=${PARSE_REDIS_SERVERS[$J]}
  echo "dispatching to $PS"
  mkdir -p $WD
  COMMAND="qsub -N fPreComp-$i-$NUM_SHARDS \
    -o $WORKING_DIR/sge-logs \
    scripts/precompute-features/extract-one-shard.sh \
      $WD \
      $JAR_STABLE \
      $i \
      $NUM_SHARDS \
      $PS"
   echo $COMMAND >>$WORKING_DIR/commands.txt
   eval $COMMAND
done


############################################################################################
### MERGE MANY SHARDS INTO A COHERENT ALPHABET
python -u scripts/precompute-features/bialph-merge-pipeline.py | tee /tmp/merge-job-launch-log.txt


############################################################################################
### COMPUTE INFORMATION GAIN

# TODO Need to utilize the ignoreSentenceIds option in InformationGain and
# InformationGainProducts to ignore the test set while computing IG.

# 1) For each template
WD=/export/projects/twolfe/fnparse-output/experiments/precompute-features/propbank/sep14b
mkdir -p $WD/ig/templates
mkdir -p $WD/ig/templates/sge-logs

JAR=target/fnparse-1.0.6-SNAPSHOT-jar-with-dependencies.jar
JAR_STABLE=$WD/ig/templates/fnparse.jar
echo "copying the jar to a safe place..."
echo "    $JAR"
echo "==> $JAR_STABLE"
cp $JAR $JAR_STABLE

#TEMPLATE_IG_FILE=$WD/ig/template-ig.txt
#java -Xmx19G -cp $JAR \
#  -DfeaturesParent=$WD/coherent-shards/features/ \
#  -DfeaturesGlob="glob:**/*" \
#  -DtopK=1000 \
#  -DoutputFeatures=$TEMPLATE_IG_FILE \
#  edu.jhu.hlt.fnparse.features.precompute.InformationGain

# Do a 1/10 estimate of 1/10 of the data and average the answers (should take 1/10th the time but be approximate)
N=10
mkdir -p $WD/ig/templates/split-$N-filter10
for i in `seq $N | awk '{print $1-1}'`; do
  qsub -N split$N-$i-filter10-ig-template \
    -o $WD/ig/templates/sge-logs \
    scripts/precompute-features/compute-ig.sh \
      $WD/coherent-shards/features \
      "glob:**/*1${i}.txt.gz" \
      $WD/ig/templates/split-$N-filter10/shard-${i}.txt \
      $JAR_STABLE
done

# Do a 1/10 estimate and average the answers (should take 1/10th the time but be approximate)
N=10
mkdir -p $WD/ig/templates/split-$N
for i in `seq $N | awk '{print $1-1}'`; do
  qsub -N split$N-$i-ig-template \
    -o $WD/ig/templates/sge-logs \
    scripts/precompute-features/compute-ig.sh \
      $WD/coherent-shards/features \
      "glob:**/*${i}.txt.gz" \
      $WD/ig/templates/split-$N/shard-${i}.txt \
      $JAR_STABLE
done

# Do a full/proper estimate
qsub -N full-ig-template \
  -o $WD/ig/templates/sge-logs \
  scripts/precompute-features/compute-ig.sh \
    $WD/coherent-shards/features \
    "glob:**/*" \
    $WD/ig/templates/split-$N/shard-${i}.txt \
    $JAR_STABLE


### After those jobs finish, average things
python scripts/precompute-features/average-ig.py \
  $WD/ig/templates/split-$N/shard-* \
  >$WD/ig/templates/split-$N/average.txt


# 2) For prodcuts of templates, filtered by top K products ranked by product of template IG
# NOTE: The more shards the more products we compute on
TEMPLATE_IG_FILE=$WD/ig/templates/split-$N/average.txt
PROD_IG_WD=$WD/ig/products
mkdir -p $PROD_IG_WD/ig-files
mkdir -p $PROD_IG_WD/sge-logs
cp target/fnparse-1.0.6-SNAPSHOT-jar-with-dependencies.jar $PROD_IG_WD/fnparse.jar
NUM_SHARDS=200
FEATS_PER_SHARD=500
for i in `seq $NUM_SHARDS | awk '{print $1 - 1}'`; do
  qsub -N prod-ig-$i -o $PROD_IG_WD/sge-logs \
    ./scripts/precompute-features/compute-ig-products.sh \
      $i \
      $NUM_SHARDS \
      $FEATS_PER_SHARD \
      $TEMPLATE_IG_FILE \
      $WD/coherent-shards/features \
      "glob:**/*" \
      $WD/coherent-shards/alphabet.txt.gz \
      $PROD_IG_WD/ig-files/shard-${i}-of-${NUM_SHARDS}.txt \
      $PROD_IG_WD/fnparse.jar
done


# vim: set syntax=sh
