#!/usr/bin/env python

import collections
import heapq
import json
import math
import numpy as np
import operator
import os
import pickle
import random
import redis
import scipy
from sklearn.linear_model import Ridge
import signal
import socket
import subprocess
import sys
import time
import uuid
import xml.etree.ElementTree as ET

class CommonEqualityMixin(object):
  def __eq__(self, other):
    return isinstance(other, self.__class__) and self.__dict__ == other.__dict__
  def __ne__(self, other):
    return not self.__eq__(other)

# sorry, global
template_info = None
FAILED_SCORE = 0.0
SER_Q_PATH = None
SER_Q = None

class Feature(CommonEqualityMixin):
  ''' uses strings as templates '''
  def __init__(self, label, templates=[]):
    if type(templates) is str:
      templates = [templates]
    assert type(label) is str
    assert type(templates) is list
    self.label = label
    self.templates = templates
    self.templates.sort()

  def __str__(self):
    x = '*'.join(self.templates)
    return "%s*%s" % (self.label, x)

  # TODO relies on a global variable
  def estimate_cardinality(self):
    c = template_info.label_cardinality[self.label]
    assert c is not None
    for t in self.templates:
      ct = template_info.template_cardinality[t]
      assert ct is not None
      c *= ct
    assert type(c) is int or type(c) is long
    return c

  def propose_modification(self, label_templates, basic_templates):
    # TODO prioritize whether you should be growing or shrinking this feature
    p_change_label = 0.1
    if len(label_templates) > 1 and random.random() < p_change_label:
      # change the label
      ys = [y for y in label_templates if y != self.label]
      return Feature(random.choice(ys), self.templates)
    else:
      # the higher the estimated cardinality, the greater the chance of
      # removing a template
      # e.g. cardinality = 100  -> p(shrink) = 0.500
      #      cardinality = 4    -> p(shrink) = 0.167
      #      cardinality = 400  -> p(shrink) = 0.667
      #      cardinality = 4000 -> p(shrink) = 0.863 (only if #templates > 1 though)
      card = pow(self.estimate_cardinality(), 0.5)
      p_shrink = card / (card + 10.0)
      if len(self.templates) > 1 and random.random() < p_shrink:
        # add a new template
        xs = [x for x in basic_templates if x not in self.templates]
        if len(xs) > 0:
          return Feature(self.label, self.templates + [random.choice(xs)])
        else:
          raise Exception('cant extend this Feature any further')
      else:
        # remove a template
        assert len(self.templates) >= 2
        tmpl = random.sample(self.templates, len(self.templates) - 1)
        return Feature(self.label, tmpl)


class FeatureSet(object):
  def __init__(self, features=[], derived_from=None):
    self.features = features
    self.derived_from = derived_from

  def __eq__(self, other):
    return isinstance(other, self.__class__) and self.features == other.features

  def __ne__(self, other):
    return not self.__eq__(other)

  def estimate_cardinality(self):
    ''' assumes that features never overlap '''
    c = 1
    for f in self.features:
      cf = f.estimate_cardinality()
      assert cf is not None and (type(cf) is int or type(cf) is long)
      c += cf
    return c

  def propose_modification(self, label_templates, basic_templates):
    ''' returns a new FeatureSet derived from this feature set '''
    debug = True
    card = self.estimate_cardinality()
    l = 0.5
    p_add_feature = l * 100.0 / (100.0 + pow(card, 0.4)) \
      + (1.0 - l) * (2.0 / (2.0 + len(self.features)))
    if random.random() < p_add_feature:
      f = self.new_feature(label_templates, basic_templates)
      if debug:
        print '[propose_modification] len(self.features) =', \
          len(self.features), 'cardinality =', card, 'p_add_features =', p_add_feature
        print '[propose_modification] new feature:', f
      return FeatureSet(self.features + [f], derived_from=self)
    else:
      # modify and replace a feature that is already in this feature set
      try:
        i = random.randrange(len(self.features))
        f = self.features[i].propose_modification(label_templates, basic_templates)
        if debug:
          print '[propose_modification] len(self.features) =', \
            len(self.features), 'cardinality =', card, 'p_add_features =', p_add_feature
          print '[propose_modification] modify existing feature:', self.features[i], '=>', f
        return FeatureSet([f] + self.features[:i] + self.features[i+1:], derived_from=self)
      except:
        return self.propose_modification(label_templates, basic_templates)

  def new_feature(self, label_templates, basic_templates):
    ''' creates a feature that is not already in this feature set '''
    y = random.choice(label_templates)
    # prefer adding features with small number of templates
    xn = 1
    while random.random() < 0.1 and xn < len(basic_templates):
      xn = xn + 1
    xs = random.sample(basic_templates, xn)
    f = Feature(y, xs)
    if f in self.features:
      return self.new_feature(label_templates, basic_templates)
    else:
      return f

  def __str__(self):
    return "<FeatureSet %s>" % (', '.join([str(x) for x in self.features]))

  def config_string(self):
    ''' returns a description to be passes to an experiment '''
    return ' + '.join([str(x) for x in self.features])


class ConfigQueue(object):
  '''
  generates FeatureSets on a priority queue and
  estimates (predicts) the utility of a feature set
  '''
  def __init__(self, beam_size, labels, basic_templates):
    print '[ConfigQueue init] beam_size =', beam_size
    print '[ConfigQueue init] labels =', labels
    print '[ConfigQueue init] basic_templates =', basic_templates
    assert len(labels) >= 1
    assert len(basic_templates) >= 1
    self.beam_size = beam_size
    self.label_templates = labels
    self.basic_templates = basic_templates

    # feature weights that comprise the score function
    self.weights = collections.defaultdict(float)
    self.intercept = 0.0

    # tuples of (score, FeatureSet) representing the observed scores
    self.scores = []

    # a config -> FeatureSet mapping for all explored FeatureSets
    self.config2fs = {}
    self.config2score = {}  # config -> float score
    self.beam = []  # tuples of (score, FeatureSet), like self.scores
    for x in basic_templates:
      for y in labels:
        f = Feature(y, x)
        fs = FeatureSet(features=[f], derived_from=None)
        self.beam.append((0.0, fs))
        self.config2fs[fs.config_string()] = fs
    assert len(self.beam) >= self.beam_size

  def pick(self, scored_feature_sets, greediness=1.0):
    '''
    makes a greedy but random selection from scored_feature_sets
    scored_feature_sets should be a list of (score, FeatureSet)
    '''
    debug = False
    # find the max score
    m = max([x[0] for x in scored_feature_sets])
    weights = []
    z = 0.0
    for s, fs in scored_feature_sets:
      regret = (m - s) * 10.0
      w = math.exp(-regret * greediness)
      weights.append(w)
      z += w
    t = random.random() * z
    if debug:
      print '[pick] z =', z, 't =', t
    c = 0.0
    for i in range(len(weights)):
      c += weights[i]
      if debug:
        print '[pick]', scored_feature_sets[i][1], 'has score', c
      if c >= t:
        fs = scored_feature_sets[i][1]
        print '[pick] chose', fs
        return fs
    print '[pick] scored_feature_sets:', scored_feature_sets
    print '[pick] weights:', weights
    assert False

  def ensure_full_beam(self, gen_from=None):
    print '[ensure_full_beam] len(beam) =', len(self.beam), 'len(scores) =', len(self.scores)
    tries = 0
    max_tries = 20 * self.beam_size
    while len(self.beam) < self.beam_size and tries < max_tries:
      tries += 1
      # generate a new feature set to put on the beam
      if gen_from:
        fs_extend = gen_from
      else:
        assert len(self.scores) > 0
        g = 1.0 * (max_tries - tries) / float(max_tries)
        fs_extend = self.pick(self.scores, greediness=g)
      fs = fs_extend.propose_modification(self.label_templates, self.basic_templates)
  
      # check if fs is already on the beam
      on_beam = False
      for bs, bfs in self.beam:
        if fs == bfs:
          on_beam = True
          break

      # store the new feature set
      if not on_beam:
        config = fs.config_string()
        if config not in self.config2fs:
          self.config2fs[config] = fs
          score = self.score(fs)
          self.beam.append((score, fs))
          print '[ensure_full_beam] adding', config, 'with score', score
        # otherwise we've already tried this config/fs

  def update_weights(self):
    ''' minimizes squared error on self.scores '''
    assert len(self.scores) > 0
    # make a feature alphbet
    s2i = {}
    i2s = []
    features = []
    for score, fs in self.scores:
      x = self.features(fs)
      features.append(x)
      for s in x:
        if s not in s2i:
          s2i[s] = len(s2i)
          i2s.append(s)
    d = len(i2s)
    # make numeric vectors for numpy to solve
    A = []
    b = []
    for i, x_str in enumerate(features):
      y = self.scores[i][0]
      x = np.zeros(d)
      for s in x_str:
        x[s2i[s]] += 1.0
      A.append(x)
      b.append(y)
    # call sklearn
    # NOTE you have to use regularized least square or else things
    # will almost certainly be co-linear and you'll get wack solutions
    m = Ridge(alpha=10.0, fit_intercept=True).fit(A, b)
    w = m.coef_
    self.intercept = m.intercept_
    # convert w back into string to double dict
    self.weights = collections.defaultdict(float)
    for i in range(len(w)):
      self.weights[i2s[i]] = w[i]
    #print '[update_weights] intercept=', self.intercept, 'weights =', self.weights

  def highest_rated_weights(self, reverse, k=10):
    return sorted(self.weights.items(), key=operator.itemgetter(1), reverse=reverse)[:k]

  def show_state(self, k=10):
    print '[show_state]', len(self.weights), \
      'weights for', len(self.scores), 'observations'
    print 'biggest (positive) weights:'
    for feat, weight in self.highest_rated_weights(True, k):
      print "\t%.3f\t%s" % (weight, feat)
    print 'biggest (negative) weights:'
    for feat, weight in self.highest_rated_weights(False, k):
      print "\t%.3f\t%s" % (weight, feat)
    print 'best feature sets:'
    for score, fs in sorted(self.scores, key=operator.itemgetter(0), reverse=True)[:k]:
      print "\t%.3f\t%s" % (score, fs.config_string())

  # TODO support non-binary features (reutrn type must change to dict or list of tuples)
  def features(self, feature_set):
    meta = []
    c = feature_set.estimate_cardinality()
    c = math.floor(pow(c, 0.4) / 30)
    meta.append('card_feats=' + str(c))
    meta.append('num_feats=' + str(len(feature_set.features)))
    min_c = 999
    max_c = 0
    labels = set()
    for feat in feature_set.features:
      c = feat.estimate_cardinality()
      c = math.floor(pow(c, 0.5) / 100)
      if c < min_c: min_c = c
      if c > max_c: max_c = c
      labels.add(feat.label)
      meta.append('label=' + feat.label) # only use if more than one label
      meta.append('feat=' + str(feat))
      meta.append('template_arity=' + str(len(feat.templates)))
      for template in feat.templates:
        meta.append('template=' + template)
        meta.append('template=' + template + ',label=' + feat.label)
    meta.append('label_set=' + '_'.join(sorted(list(labels))))
    meta.append('template_min_card=' + str(min_c))
    meta.append('template_max_card=' + str(max_c))
    if feature_set.derived_from:
      c = feature_set.derived_from.config_string()
      s = self.config2score[c]
      if s:
        # don't know the scale, convert to scientific notation, take 2 significant digits
        meta.append("derived_from_score=%.2g" % (s))
    return meta

  def score(self, feature_set):
    ''' returns the estimated utility of a feature set '''
    # TODO add a feature for derived_from's score
    score = self.intercept
    for feat in self.features(feature_set):
      score += self.weights[feat]
    return score

  def observe_score(self, score, config):
    ''' updates the model and the beam given the results of an experiment '''
    print '[observe_score] score =', score, 'config =', config
    assert type(score) is float
    assert type(config) is str
    feature_set = self.config2fs[config]
    assert type(feature_set) is FeatureSet
    if config in self.config2score:
      if self.config2score[config] == FAILED_SCORE:
        print ("[observe_score] WARN: may need to sleep longer, " \
          "%s was originally deemed failed but later reported a score of %f") \
          % (config, score)
      else:
        raise Exception(config + ' had a score of ' \
          + str(self.config2score[config]) \
          + ' but you tried to observe the score ' + str(score))
    self.config2score[config] = score
    # update weights
    self.scores.append((score, feature_set))
    self.update_weights()
    # re-score beam
    for i in range(len(self.beam)):
      (score, fs) = self.beam[i]
      score2 = self.score(fs)
      self.beam[i] = (score2, fs)
    # return beam to its regular state
    self.ensure_full_beam()
    heapq.heapify(self.beam)
    # print the biggest weights
    if len(self.scores) % 10 == 0:
      self.show_state()

  def pop_feature_set(self):
    ''' returns the FeatureSet which is estimated to have the highest utility '''
    # TODO make this an epsilon-greedy selection
    assert len(self.beam) > 0
    return heapq.heappop(self.beam)[1]


class SgeJobTracker(object):
  '''
  a job tracker that asks qstat for the jobs that are running
  and spawns jobs with qsub
  '''
  def can_submit_more_jobs(self):
    return len(self.jobs_queued()) < 30

  def jobs(self):
    '''
    name_predicate should be a lambda that takes a string (name)
    and returns true if the job should be kept.
    This method skips over any jobs that are marked as QLOGIN,
    so name_predicate need not filter those out.
    Returns a list of job names.
    '''
    #r = subprocess.check_output(['qstat'])
    #if not r:
    #  return 0
    #jobs = []
    #for line in r.split('\n')[2:]:
    #  toks = line.split()
    #  name = toks[2]
    #  if name == 'QLOGIN':
    #    continue
    #  if name_predicate(name):
    #    jobs.append(name)
    #return jobs

    # need to do this with XML to get the full names
    xml = subprocess.check_output(['qstat', '-u', 'twolfe', '-xml'])
    #print '[sge jobs] xml=' + xml
    xml = ET.fromstring(xml)
    assert xml.tag == 'job_info'
    # NOTE: wow this is really bad...
    # SGE reports *running* jobs in a list called 'queue_info'
    # and reports *queued* jobs in a list called 'job_info'
    for info_name in ['job_info', 'queue_info']:
      info = xml.find(info_name)
      assert info is not None
      # NOTE: each 'job_list' is actually a job
      # not a list of jobs as the name would suggest
      for j in info.findall('job_list'):
        #print 'j.tag', j.tag
        state = j.find('state').text    # e.g. 'r' or 'qw'
        name = j.find('JB_name').text
        #print '[sge jobs]', state, name
        if name == 'QLOGIN':
          continue
        yield (state, name)

  def jobs_running(self):
    ''' returns a list of job names '''
    return [name for state, name in self.jobs() if state == 'r']

  def jobs_queued(self):
    ''' returns a list of job names '''
    return [name for state, name in self.jobs() if state == 'qw']

  def spawn(self, name, args):
    cmd = ['qsub', '-N', name, 'ForwardSelectionWorker.qsub'] + args
    print '[sge spawn] cmd =', cmd
    subprocess.Popen(cmd)
    time.sleep(0.2)


class LocalJobTracker(object):
  '''
  mock job tracker which uses redis instead of qsub
  if debug is true, this will call scripts/dummy-forward-selection-job
  else this will call the actual experiment
  '''
  def __init__(self, debug=False, max_concurrent_jobs=2):
    self.key = 'dummy-job-tracker.jobs'
    self.redis = redis.StrictRedis(host='localhost', port=6379, db=0)
    self.debug = debug
    self.max_concurrent_jobs = max_concurrent_jobs

  def remove_all_jobs(self):
    ''' ensures that there are no jobs running (for testing) '''
    self.redis.delete(self.key)

  def can_submit_more_jobs(self):
    return len(self.jobs_running()) < self.max_concurrent_jobs

  def jobs_running(self):
    # TODO this won't work in cases where jobs die!
    # qsub can handle this, but to remove from redis queue, we've been assuming things finish
    return self.redis.lrange(self.key, 0, -1)

  def set_job_done(self, name):
    print '[set_job_done] name=' + name
    self.redis.lrem(self.key, 0, name)

  def jobs_queued(self):
    return []

  def spawn(self, name, args):
    self.redis.rpush(self.key, name)
    if self.debug:
      subprocess.Popen(['scripts/dummy-forward-selection-job'] + args)
    else:
      cp = subprocess.check_output(['find', 'target/', '-iname', '*.jar']).strip()
      cp = ':'.join([x.strip() for x in cp.split('\n')])
      cmd = ['java', '-Xmx4G', '-ea', '-cp', cp]
      cmd += ['-XX:+UseG1GC', '-XX:G1ReservePercent=2', '-XX:ConcGCThreads=1', '-XX:ParallelGCThreads=1']
      cmd.append('edu.jhu.hlt.fnparse.experiment.ForwardSelectionWorker')
      cmd += args
      print 'about to spawn:', cmd
      subprocess.Popen(cmd)


# TODO rip out the config_q part of this, daemonize it, and release it as a robust
# version of qsub (i.e. watches to make sure jobs finish, restarts them if not)
class ForwardSelection:
  def __init__(self, name, working_dir, job_tracker, config_q, poll_interval=3.0, \
      redis_config={'channel':'forward-selection', 'host':'localhost', 'port':'6379', 'db':'0'}):
    print '[ForwardSelection] attempting to use redis server at', redis_config
    self.name = name
    self.working_dir = working_dir  # where workers are allowed to dump results
    self.redis_config = redis_config
    self.job_tracker = job_tracker  # talks to qsub
    self.config_q = config_q
    self.name2config = {}           # stores (job name -> config string)
    self.config2name = {}
    self.poll_interval = poll_interval
    self.dispatched = set()         # names of the jobs that (should be) running

  def parse_message(self, data):
    '''
    parses a message from the experiment over redis pubsub
    and returns a tuple of (config, score)
    '''
    toks = data.split('\t')
    assert len(toks) == 2
    score = float(toks[0])
    config = toks[1]
    return (config, score)

  def start_job(self):
    ''' returns a unique job name and updates self.name2config '''
    fs = self.config_q.pop_feature_set()
    if fs:
      name = "fs-%s-%d" % (self.name, len(self.name2config))
      config = fs.config_string()
      assert name not in self.name2config
      assert config not in self.config2name
      self.name2config[name] = config
      self.config2name[config] = name
      print '[start_job] name=' + name + ' config=' + config
      wd = os.path.join(self.working_dir, name + '-wd')
      if not os.path.isdir(wd):
        os.mkdir(wd)
      cmd = []
      cmd.append(self.redis_config['channel'])
      cmd.append(self.redis_config['host'])
      cmd.append(self.redis_config['port'])
      cmd.append(wd)
      cmd.append(config)
      self.job_tracker.spawn(name, cmd)
      self.dispatched.add(name)
      return name
    else:
      return None

  def can_submit_more_jobs(self):
    return self.job_tracker.can_submit_more_jobs() and len(self.config_q.beam) > 0

  def run(self):
    r = redis.StrictRedis(host=self.redis_config['host'], port=self.redis_config['port'], db=self.redis_config['db'])
    p = r.pubsub(ignore_subscribe_messages=True)
    p.subscribe(self.redis_config['channel'])
    perf_file = open(os.path.join(self.working_dir, 'perf.txt'), 'w')
    while True:
      # try to dispatch new jobs
      if self.can_submit_more_jobs():
        if not self.start_job():
          print 'Done!'
          break
      else:
        # check for results
        message = p.get_message()
        print 'received message:', message
        if message:
          (config, score) = self.parse_message(message['data'])
          name = self.config2name[config]
          print name, '/', config, 'finished successfully with a score', score
          perf_file.write("%f\t%s\t%s\n" % (score, name, config))
          perf_file.flush()
          self.config_q.observe_score(score, config)
          # remove this jobs from dispatched
          if type(self.job_tracker) is LocalJobTracker: # for debugging
            self.job_tracker.set_job_done(name)
          try:
            self.dispatched.remove(name)
          except:
            print name + ' was not in dispatched, we gave up on this job as failed previously'
        else:
          # check if any jobs died
          r = set(self.job_tracker.jobs_running() + self.job_tracker.jobs_queued())
          failed = set([name for name in self.dispatched if name not in r])
          failed_configs = set()
          for name in failed:
            config = self.name2config[name]
            print 'failed:', name, config
            assert config not in failed_configs
            failed_configs.add(config)
            self.config_q.observe_score(FAILED_SCORE, config)
            self.dispatched.remove(name)
          else:
            print 'everything is running nicely:', len(r)
            time.sleep(self.poll_interval)
    perf_file.close()


class TemplateInfo(object):
  def __init__(self, filename):
    print '[TemplateInfo init] reading template information from', filename
    self.filename = filename
    self.template_cardinality = {} # string -> int
    self.basic_templates = [] # list of strings (names)
    f = open(filename, 'r')
    for line in f:
      toks = line.strip().split('\t')
      template_name = toks[0]
      template_card_frameid = int(toks[1])
      template_card_rolelab = int(toks[2])
      assert template_name not in self.template_cardinality
      if template_card_rolelab > 1:
        self.template_cardinality[template_name] = template_card_rolelab
        self.basic_templates.append(template_name)
    f.close()
    # NOTE: the cardinality of frame is w.r.t. how many frames could possibly
    # be evoked at a give target, which is much less than the number of total frames
    self.label_cardinality = {'intercept':1, 'frame':10, 'frameRole':50, 'role':15}
    print '[TemplateInfo init] done'

def save_config_q_handler(signum, frame):
  if SER_Q is not None and SER_Q_PATH is not None:
    print 'saving config queue to', SER_Q_PATH
    pickle.dump(SER_Q, open(SER_Q_PATH, 'wb'))
  sys.exit(0)

def main(template_info_filename, ser_q_filename, deser_q_filename=None):
  # Read in the set of templates from disk (dumped by BasicFeatureTemplates)
  global template_info
  template_info = TemplateInfo(template_info_filename)
  #template_info = TemplateInfo('experiments/forward-selection/basic-templates.txt')
  beam_size = 30
  local = False
  #labels = ['frame', 'intercept']
  labels = ['frameRole', 'role', 'intercept']

  # create the config queue
  if deser_q_filename is None:
    print 'creating empty config queue'
    q = ConfigQueue(beam_size, labels, template_info.basic_templates)
  else:
    print 'loading config queue from', deser_q_filename
    f = open(deser_q_filename, 'rb')
    q = pickle.load(f)
    f.close()
    q.show_state()

  # make sure config queue is setup to be saved on SIGINT
  global SER_Q_PATH
  global SER_Q
  SER_Q_PATH = ser_q_filename
  SER_Q = q
  signal.signal(signal.SIGINT, save_config_q_handler)

  # create the job tracker
  if local:
    jt = LocalJobTracker(debug=False, max_concurrent_jobs=2)
    jt.remove_all_jobs()
  else:
    jt = SgeJobTracker()

  #working_dir = 'experiments/forward-selection/frameId'
  working_dir = 'experiments/forward-selection/roleLabeling'
  redis_config = {
    'channel': 'forward-selection',
    'host': socket.gethostname(),
    'port': '6379',
    'db': '0'
  }
  fs = ForwardSelection('test', working_dir, jt, q, \
    redis_config=redis_config, poll_interval=3.0)

  os.system('rm logging/forward-selection/*')
  if not local:
    print 'cleanup...'
    os.system('kill-all-jobs')
    time.sleep(2)
  print 'starting...'
  fs.run()

if __name__ == '__main__':
  if len(sys.argv) == 3:
    main(sys.argv[1], sys.argv[2])
  elif len(sys.argv) == 4:
    main(sys.argv[1], sys.argv[2], deser_q_filename=sys.argv[3])
  else:
    print 'please provide:'
    print '1) a path to a basic template cardinality file (e.g. experiments/forward-selection/basic-templates.txt)'
    print '2) a path to save the ConfigQueue to'
    print '3) [optional] a place to load a ConfigQueue from'
    sys.exit(-1)


# every experiment gets a human readable, but uniq name, e.g. 'frameId-forwardSelection-2014-10-09'
# every job gets a uniq id, a UUID
# the experiment name is used as a key in redis
# the values associated with it json that represent information about the run
# e.g. some run might be:
#{
#  jobId : '2b26c166-4f56-11e4-99fc-7c7a9146f5f0',
#  command : 'java -Xmx4G edu.jhu.hlt.fnparse.foo.Bar',
#  jar_hash : 'a5ff9934d159216c49a2b386edba062a15100907',
#  completed : '2014-10-09 01:24:09',
#  host : 'ch12',
#  params : {foo = 42, bar = 'baz'}
#  results : {targetMicroF1 = 0.733, runtime : 934.2}
#}




