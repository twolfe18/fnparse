\documentclass[12pt]{article}
\usepackage{acl2017}
\usepackage{hyperref}

\title{Gripes}
\author{Travis Wolfe}
\date{\today}

\begin{document}
\maketitle

\section{Introduction}
% I'm writing this essay after coming across some misleading research.
% If this is the only such situation, then this is not worth writing,
% but I believe that this situation is a prototype,
% indicative of our community's attitude
% towards empirical research and ``selling your work" in general.
% 
% I'm going to focus primarily on the work of \newcite{iyyer:2014},
% but I want to make it clear from the outset that there are very good aspects
% of this work, both in general (methods), and in relation to this essay.
% \newcite{iyyer:2014} provided the data necessary to take a critical look at
% their work, which is not common.

I'm writing this paper to ask for the retraction of the work of
\newcite{iyyer:2014}.
I will lay out the misleading aspects of this paper which lead me
to this conclusion.



\section{Problems with the Experimental Design}
The title of \newcite{iyyer:2014} is
``A Neural Network for Factoid Question Answering over Paragraphs".
The task they are concerned with is Quiz Bowl, a trivial competition
where contestants are given a paragraph describing an answer (usually
people, books, and historical events in their case). Contestants can
``buzz-in" before the entire question is stated, similar to Jeopardy.
This task has many interesting aspects, ranging from answer-type detection,
estimating the utility of evidence, and retrieving candidiate answers.

The dataset was constructed
by taking quiz bowl questions and answers
(originally by \newcite{Boyd-Graber:2012}
and then expanded by \newcite{iyyer:2014}).
The answers given were just strings.
If evaluated this way, the problem would be difficult.
To simplify, the dataset construction included a step to link
a string answer to a Wikipedia page.
This Wikipedia page was then assumed as a vocabulary term for which
an embedding was learned.
This is the first hint that this paper is not intent on solving any
real problem, as opposed to ``selling" a method.
Neither contestants, nor any reasonable system, would have a pruned
set of correct answers available to them.
Additionally, this is by its very nature not a closed-class problem
like part of speech tagging, where such an assumption is reasonable.

% CONSTRUCTED the dataset using string matching techniques to get
% from the answer (Thomas Mann) to a wikipedia page. First, this is CHEATING.
% Saying that your method "found" this answer is INSANE, because your answer
% only chose it from a set chosen by STRING MATCHING TECHNIQUES (which
% happened to be cheating).

The next worrying aspect of this work is that the data is pruned to
exclude any question-answer pairs where the answer did not appear at
least $k$ times (in the paper it was stated as 6, but in the data
I downloaded it appears to be 5).
This is a very limiting assumption, which ignores between 76 and 80\%
of the questions. It is explicitly stated that this is so that their model
has more training data to perform better, putting them at an advantage over
the baseline.
Most worrying however is the fact that the way the train-dev-test split
was performed, every single answer in the test set appeared at least once
in both the training and dev set.
This detail was vaguely hinted at by a comment about zero-shot learning
which was describing the filtering by answer count proceedure
and mentioned no where in the train-dev-test split proceedure description.


\section{Problems with the Analysis}
Their results indicate that their method works better than
bag-of-words, bag-of-dependencies, and IR-based models.
See their paper for descriptions of these methods.
In \S5 they discuss their model and what it gets right and
wrong, especially compared to other models.

\paragraph{Thomass Mann}
In \S5.2 they say:
\begin{quote}
We look closely at the first sentence from a
literature question about the author Thomas
Mann: ``He left unfinished a novel whose title
character forges his father’s signature to get
out of school and avoids the draft by feigning
desire to join".
All baselines, including ir-wiki, are unable
to predict the correct answer given only this
sentence. However, qanta makes the correct
prediction. The sentence contains no named
entities, which makes it almost impossible for
bag of words or string matching algorithms to
predict correctly.
\end{quote}

Firstly, note that this example is not in the released dataset.
I will chalk this up to a data munging mistake which will be promptly fixed.

Indeed the lack of named entities does make prediction difficult.
They imply that \textsc{qanta} is clever enough to figure out the
answer in spite of this.
I made an attempt to line up evidence in the prompt with evidence
in either the training data (\textsc{quanta}) or the Wikipedia page (\textsc{quanta+IR-wiki}) for Thomas Mann.

With respect to the former, the only two words which appear in both the
prompt and the training data are ``character" (which is very common, appearing
in the prompts for 1093 distinct answers, as many answers are authors)
and ``unfinished" (appearing with 86 distinct answers).
None of the prompts mention a ``draft", or ``enlisting", or ``joining", or ``army",
a ``signature", a ``forging", or a ``faking" (checking for lemma matches).\footnote{There is one
instance of an ``impersontation", but it is referring to a different book and not the ``forging".}
The term ``school" (appearing with 287 distinct answers) is mentioned once, but not referring to the
same book as described in the prompt above.
It is possible that \textsc{qanta}'s neural model is more clever than I am, but I could not
find any evidence supporting this prediction from the evidence \textsc{qanta} was given.
From this I conclude that the odds that \textsc{qanta} got this answer right by some means
other than luck is probably no higher than $\frac{1}{86}$.

% caligula question_data $ grep -i unfinished questions.csv | awk -F"," '{print $3}' | sort -u | wc -l
% 86
% caligula question_data $ grep -i character questions.csv | awk -F"," '{print $3}' | sort -u | wc -l
% 1093

For the latter, the wikipedia page, it contains discussions of:
\begin{itemize}
\item ``father" appears 3 times, but none of them referring to the father in the described book
\item ``school" appears once, not referring to the story
\item ``unfinished" (or any variant of ``finish") is used once to describe 3 books, none of which are the referred to book:
  \href{https://en.wikipedia.org/wiki/Lotte\_in\_Weimar:\_The\_Beloved\_Returns}{The Beloved Returns},
  \href{https://en.wikipedia.org/wiki/Doctor\_Faustus\_(novel)}{Doctor Faustus},
  and \href{https://en.wikipedia.org/wiki/Confessions\_of\_Felix\_Krull}{Confessions of Felix Krull}.
\item None of the other terms mentioned above can be found on the Wikipedia page.
\end{itemize}




\paragraph{John Quincy Adams}
The next example offered is the following prompt:
\begin{quote}
Q: he also successfully represented the amistad
   slaves and negotiated the treaty of ghent and
   the annexation of florida from spain during his
   stint as secretary of state under james monroe

A: john quincy adams
\end{quote}

They say that:
\begin{quote}
Our next example, the first sentence in Table
2, is from the first position of a question
on John Quincy Adams, which is correctly answered
by only qanta. The bag of words
model guesses Henry Clay, who was also a Secretary
of State in the nineteenth century and
helped John Quincy Adams get elected to the
presidency in a “corrupt bargain”. However,
the model can reason that while Henry Clay
was active at the same time and involved in
the same political problems of the era, he did
not represent the Amistad slaves, nor did he
negotiate the Treaty of Ghent.
\end{quote}

First, this is odd since this is a question from the train set
rather than dev.

Second, their claim that Clay had nothing to do with the Ghent treaty is
not true according to the Wikipedia article they released:
``Clay helped negotiate the Treaty of Ghent and signed it on December 24, 1814."
and two question-answer instances in the train set.

Third, the only remaining clue which discriminates between Adams and Clay is
involvement with the Amistad (a Spanish vessel).
This is particularly interesting because the Amistad is only mentioned with
two entities in the all questions, once in the training set with Adams as the
answer, and once in the test set with Martin Van Buren as the answer.
If \textsc{qanta} did indeed get this right, it could have only been on a
train and test on the train set configuration.



\paragraph{John Cabot}
The next example they give is about John Cabot which no system
gets right. The only reason this is strange is because it is
not in the dataset I downloaded.

% The next example is:
% \begin{quote}
% Consider this question about the Italian explorer
% John Cabot: ``As a young man, this
% native of Genoa disguised himself as a Muslim
% to make a pilgrimage to Mecca".
% While it is obvious to human readers that
% the man described in this sentence is not actually
% a Muslim, qanta has to accurately model
% the verb disguised to make that inference. We
% show the score plot of this sentence in Figure 7.
% The model, after presumably seeing many instances
% of muslim and mecca associated with
% Mughal emperors, is unable to prevent this
% information from propagating up to the root
% node. On the bright side, our model is able to
% learn that the question is expecting a human
% answer rather than non-human entities like the
% Umayyad Caliphate.
% \end{quote}
% 
% John Cabot is not in the dataset I downloaded, neither as an quesiton/answer or
% as a wiki page.  His wikipedia page only mentions Mecca once, and does not
% mention ``disguise" or ``pilgramage".  This is not a failure per se. First, no
% system can get it right since its not in the universe of answers.  Second,
% there isn't really enough information, even for a human to get this right.







\paragraph{Howl} This appears in table 2:
\begin{quote}
Q: this work refers to people who fell on their
  knees in hopeless cathedrals and who jumped
  off the brooklyn bridge

A: howl, the tempest, paradise lost
\end{quote}

Neither the Wikipedia page nor any of the training data
mention a cathedral, Brooklyn, a bridge, jumping, or falling.



\paragraph{Nagasaki}
The one example for which it is at least plausible that \textsc{qanta}
worked as claimed was this:
\begin{quote}
Q: despite the fact that twenty six martyrs were
  crucified here in the late sixteenth century it
  remained the center of christianity in its country

A: nagasaki, guadalcanal, ethiopia
\end{quote}

This question-answer is not in the released dataset.

The Wikipedia page does mention Christianity many times.
It describes Nagasaki as the ``center of rangaku" and ``center of heavy industry"
but not the ``center of Christianity".
The term ``martyrs" is perhaps a giveaway, even through it appears with 47 entities
in the question data.
%Give-away is "martyrs", but differs in ``Twenty-six":wiki vs ``twenty six":prompt. It does say
%``They are known as the Twenty-six Martyrs of Japan. ... They were executed by public crucifixion in Nagasaki."

%So oddly, I think this works because it is good at lemma matching, which is pretty close to string matching, which they criticise.

Interestingly, in the prompt ``twenty six" is two words, but in the Wikipedia
page it is hyphentated as ``Twenty-six".
Stanford CoreNLP's online demo does not tokenize apart this hyphenation.
If \textsc{qanta} is able to learn distinct representations for ``twenty six" and ``Twenty-six"
which can be used to get this question right, I believe it is a powerful result which deserves
further investigation.



%%%%% At this point I'm pretty sure that a naive-bayes baseline would come pretty darn close to their results.
 
% \begin{quote}
% Q: this novel parodies freudianism in a chapter
%   about the protagonist ’s dream of holding a
%   live fish in his hands
% 
% A: catch-22, billy budd, the ambassadors, all my sons
% \end{quote}
% 
% There is no way to answer this with the wikipedia page.



\section{Why did Naive Bayes fail?}
In looking at the data, there appear to be many examples which
seem like they could be selected from the pool of 2342 answers
rather easily using plain string matching.

One example which was given as an instance that no system got
right is:
\begin{quote}
Q a contemporary of elizabeth i he came to power
  two years before her and died two years later
A grover cleveland, benjamin harrison, henry cabot lodge
\end{quote}

The first sentence isn't enough on its own, but given the rest of the clue
it is relatively easy to determine the correct answer is either Akbar (incorrect)
or Shah Jahan (correct) by taking wikipedia pages with the most rare terms,
in this case ``Mughal", ``Hindu", ``Deccan", ``1605".

\begin{verbatim}
3864,test,Shah Jahan,History,
A contemporary of Elizabeth I, he came to power two years before her and died two years later.
As emperor, he established a strongly centralized government and extended his kingdom into the Deccan.
An advocate of religious tolerance, he removed the special tax upon non-Muslims, forbade slave raids upon the Hindus, and himself twice married Hindu princesses.
Ruling from 1556 to 1605, he also conquered Afghanistan and Baluchistan.
FTP name this 16th century emperor, the greatest of the Mughals
\end{verbatim}

% Terms in both the question and wiki page:
% NO: Elizabeth, Afghanistan, Baluchistan, 1556
% YES: Mughals, Hindu, Deccan, 1605

Given how easy it is to get down to a very small set of potential
answers by just boolean queries,
how is it that Naive Bayes does so poorly?
They cite previous work showing NB doesn't work \cite{Boyd-Graber:2012}.

% Besting the Quiz Master: Crowdsourcing Incremental Classification Games
% Jordan Boyd-Graber, Brianna Satinoff, He He, and Hal Daume III
% http://www.aclweb.org/anthology/D12-1118

I believe the modeling choice which leads to bad performance is:
\begin{quote}
Features were taken to be the 25,000 most frequent tokens and bigrams that were
not stop words; features were extracted from the Wikipedia text in the same
manner as from the question tokens.
\end{quote}

All of the signal is below the top-25K vocab.




\begin{figure*}
\begin{center}
\begin{tabular}{l|r|r|r|r|r|r}
                        & \multicolumn{3}{c}{History}             & \multicolumn{3}{c}{Literature}  \\
Method                  &   Pos 1     & Pos 2       & Full        & Pos 1     & Pos 2     & Full    \\
\hline
$\textsc{tfidf}_p$      &   23.1      & 48.6        & 87.4        & 19.3      & 39.6      & 93.9   \\
\textsc{qanta}          &   47.1      & 72.1        & 73.7        & 36.4      & 68.2      & 69.1    \\
\hline
$\textsc{tfidf}_{pw}$   &   ukn       & ukn         & 82.7        & ukn       & ukn       & 87.4   \\
\textsc{qanta+ir-wiki}  &   59.8      & 81.8        & 82.3        & 44.7      & 78.7      & 76.6    \\
\end{tabular}
\end{center}
\caption{Our baselines and the methods of \newcite{iyyer:2014}.}
\end{figure*}

\section{A Reasonable Baseline}
I implemented a tf-idf cosine similarity baseline.
A vector is built for every answer in the training set where
terms are taken from the concatenation of all the prompts for which
it was the correct answer.
We call this variant $\textsc{tfidf}_p$.
We implement another variant $\textsc{tfidf}_{pw}$ which builds
a tf-idf vector for the wikipedia page for the a given answer as well,
and outputs the final score as $(1+\cos(\theta_p)) (1 + \cos(\theta_w))$
where $\theta_p$ is the angle between the prompt tf-idf vector and
a given answer's prompts, and similar for $\theta_w$.
Prediction is simply an argmax over the answers observed at train time,
where we filter out any answers which are a substring of the prompt.

% baseline-dev-tfidf-iP-vP-s1.txt
% dev category=History                      right=104     total=451       23.06
% dev category=Literature                   right=107     total=555       19.28
% 
% baseline-dev-tfidf-iP-vP-s2.txt
% dev category=History                      right=219     total=451       48.56
% dev category=Literature                   right=220     total=555       39.64
% 
% baseline-dev-tfidf-iP-vP-sN.txt
% dev category=History                      right=394     total=451       87.36
% dev category=Literature                   right=521     total=555       93.87
% 
% baseline-dev-tfidf-iPW-vPW-sN.txt
% dev category=History                      right=373     total=451       82.71
% dev category=Literature                   right=485     total=555       87.39

There are at least two interesting things about these results.
First, our tf-idf method which has no trainable parameters,
performs better than \textsc{qanta} in all full-data cases,
especially when Wikipedia information is not available.
After looking at these articles and questions, this is not surprising,
since it is common, for the literature questions, to describe the characters in authors'
books, whereas this is uncommon in Wikipedia. Wikipedia tends to stick to a biography of
the author (e.g. the Wikipedia article for Thomas Mann discusses {\em Thomas Mann's} father
rather than one of his character's fathers as was done in the question example).
The history Wikipedia articles are more relevant to the questions, but the hints provided
by the training data prompts tends to be a better indicator
(its plausible that the quiz bowl question writers have a different bias
for the types of information they reveal in their clues vs what Wikipedia authors discuss).

% caligula question_data $ grep ',train,' questions.noLineRet.csv | grep -i Faustus | awk -F"," '{print $3}' | sort
% Carthage
% Christopher Marlowe
% Christopher Marlowe
% Christopher Marlowe
% Christopher Marlowe
% Christopher Marlowe
% Christopher Marlowe
% Christopher Marlowe
% Christopher Marlowe
% Doctor Faustus (novel)
% Doctor Faustus (novel)
% Doctor Faustus (novel)
% Hart Crane
% Hart Crane
% Ludwig van Beethoven
% Manichaeism
% Theoderic the Great
% Thomas Mann
% Thomas Mann
% Thomas Mann
% Thomas Mann
% Thomas Mann
% caligula question_data $ grep ',train,' questions.noLineRet.csv | grep -i Settembrini | awk -F"," '{print $3}' | sort
% The Magic Mountain
% The Magic Mountain
% The Magic Mountain
% The Magic Mountain
% The Magic Mountain
% Thomas Mann
% Thomas Mann
% Thomas Mann
% Thomas Mann
% Thomas Mann
% Thomas Mann

The next interesting aspect is the performance for \textsc{tfidf}
degrades much faster than \textsc{qanta}, such that for both the
1 and 2 sentence prompt paradigms, \textsc{qanta} wins by a large margin.
This is consistent with the idea that \textsc{qanta} can build composition
representation which captures meaning at the super-word level.
%To see if that is plausible, we look at the low-data cases which \textsc{qanta} gets
%right and \textsc{tfidf} gets wrong.
We currently do not have the data to be able to compare predictions in the
low-data case to see what \textsc{qanta} gets right and \textsc{tfidf} wrong.
% TODO I can run their system.





% TODO is it possible that they are conditioning their response on the category?
% This seems plausible that this would boost the performance at the low end, without
% affecting it at the high end.





\section{Consequences of the Mistakes}
A lot of folks thinks that neural methods are great at everything these days.
This belief is so prevalent that critical thinking is fading.
Cases like this discourage even skeptical readers due to good performance
compared to the poorly implemented baselines.
Understanding proposed methods means being critical of them,
and explaining why they work in face of this criticism.


\section{Bad Apple?}
The work of \newcite{iyyer:2015} is lax with respect to
research methods as well.
They study neural bag of words models.
One of their evaluations is on sentiment, for which they do
not include a Naive Bayes or or logitistic regression baseline.
The examples they give in table 3 are consistent with the
behavior of a vanilla bag of words model.
They also evaluate on the quiz bowl QA data described above
and get even worse results than \textsc{qanta}.
They say this is because it is an un-ordered (syntax-free)
model which requires significantly less training time.
This is not a sufficient explanation given that the \textsc{tfidf}
baseline described here is un-ordered and requires no
training time.\footnote{If computing document frequencies
counts as training, then we would have to run experiments
on the fastest way to do this. For this work, I had some word
counts laying around.}


\bibliographystyle{acl2017}
\bibliography{sources}

\end{document}

